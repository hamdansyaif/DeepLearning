{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvRnc+a1B60fR5EUNCRLhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamdansyaif/DeepLearning/blob/main/Hands-On-ML2/Chapter-06/Analisis_Chapter_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📘 **Analisis Teoretikal & Konsep Penting**\n",
        "\n",
        "---\n",
        "\n",
        "### 1. ✅ **Decision Tree Basics**\n",
        "\n",
        "* **Algoritma utama**: CART (Classification and Regression Tree)\n",
        "* Hanya menghasilkan **binary tree**\n",
        "* Dapat digunakan untuk **klasifikasi & regresi**\n",
        "* Sifat utama: **white-box model** → mudah ditafsirkan\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 📐 **Impurity Measures**\n",
        "\n",
        "#### Gini Impurity:\n",
        "\n",
        "$G_i = 1 - \\sum_{k=1}^n p_{i,k}^2$\n",
        "\n",
        "* Default di `DecisionTreeClassifier`\n",
        "* Lebih cepat dihitung\n",
        "* Cenderung mengisolasi kelas mayoritas lebih cepat\n",
        "\n",
        "#### Entropy:\n",
        "\n",
        "$H_i = -\\sum_{k=1}^n p_{i,k} \\log_2 p_{i,k}$\n",
        "\n",
        "* Lebih \"informatif\", hasilkan tree yang lebih seimbang\n",
        "* Lebih lambat dibanding Gini\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 🧱 **Hyperparameter & Regularisasi**\n",
        "\n",
        "Untuk menghindari overfitting:\n",
        "\n",
        "* `max_depth`: batas kedalaman tree\n",
        "* `min_samples_split`: minimum jumlah sample agar node bisa di-split\n",
        "* `min_samples_leaf`: minimum sample di leaf node\n",
        "* `max_leaf_nodes`: batasi jumlah leaf\n",
        "\n",
        "Pengaruh:\n",
        "\n",
        "* **Tanpa regularisasi** → overfit (tree menyesuaikan noise)\n",
        "* **Dengan regularisasi** → lebih generalisasi\n",
        "\n",
        "---\n",
        "\n",
        "### 4. 💡 **Decision Boundaries**\n",
        "\n",
        "* Boundary dari Decision Tree selalu **perpendicular ke feature axis**\n",
        "* Membuat model jadi sensitif terhadap **rotasi fitur**\n",
        "* Terlihat jelas pada data Iris dan Moons (rotasi → boundary berubah total)\n",
        "\n",
        "---\n",
        "\n",
        "### 5. 🔁 **Instabilitas Model**\n",
        "\n",
        "* Decision Tree bisa sangat sensitif: perubahan 1 instance saja bisa mengubah struktur tree\n",
        "* Solusi:\n",
        "\n",
        "  * **Set `random_state`**\n",
        "  * **Gunakan ensemble (mis. Random Forest)** untuk stabilisasi\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 **Analisis Code & Eksperimen**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Training Model Klasifikasi Awal**\n",
        "\n",
        "```python\n",
        "DecisionTreeClassifier(max_depth=2).fit(X, y)\n",
        "```\n",
        "\n",
        "* Menggunakan Iris data\n",
        "* Visualisasi pakai `plot_tree` atau `export_graphviz`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Estimasi Probabilitas**\n",
        "\n",
        "```python\n",
        "tree.predict_proba([[5.0, 1.5]])\n",
        "```\n",
        "\n",
        "* Kembalikan proporsi kelas di leaf node\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Boundary Visualization**\n",
        "\n",
        "* Menggunakan `np.meshgrid` dan prediksi pada seluruh grid\n",
        "* Sangat penting untuk memahami *decision region*\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Regularisasi**\n",
        "\n",
        "```python\n",
        "DecisionTreeClassifier(min_samples_leaf=5)\n",
        "```\n",
        "\n",
        "* Membatasi kompleksitas tree\n",
        "* Mencegah overfitting pada data seperti `make_moons`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Rotasi Data**\n",
        "\n",
        "```python\n",
        "X_rotated = X @ rotation_matrix\n",
        "```\n",
        "\n",
        "* Menunjukkan **kekurangan pohon keputusan**: tidak invarian terhadap rotasi\n",
        "* Kelemahan utama algoritma berbasis axis-aligned splitting\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Regression Tree**\n",
        "\n",
        "```python\n",
        "DecisionTreeRegressor(max_depth=2)\n",
        "```\n",
        "\n",
        "* Menggunakan target numerik (y kontinu)\n",
        "* Output leaf adalah **rata-rata** target di dalamnya\n",
        "* Cost function: **Mean Squared Error (MSE)**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **Grid Search (Exercise 7)**\n",
        "\n",
        "```python\n",
        "GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3)\n",
        "```\n",
        "\n",
        "* Cari kombinasi parameter terbaik (`max_leaf_nodes`, `min_samples_split`, `criterion`)\n",
        "* Menyediakan **model optimal**\n",
        "\n",
        "---\n",
        "\n",
        "### 🌲 **Random Forest Manual (Exercise 8)**\n",
        "\n",
        "* **1.000 pohon** dilatih pada subset acak kecil (100 instance)\n",
        "* Hasil prediksi setiap pohon di-*voting* via:\n",
        "\n",
        "```python\n",
        "from scipy.stats import mode\n",
        "mode(preds, axis=0)\n",
        "```\n",
        "\n",
        "* **Majority voting** menstabilkan hasil prediksi\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 **Analisis Visualisasi Kamu (Final Plot)**\n",
        "\n",
        "| Aspek           | Best Tree                    | Ensemble (1000 Trees)      |\n",
        "| --------------- | ---------------------------- | -------------------------- |\n",
        "| **Akurasi**     | 85–87% (bias varians tinggi) | 86–88% (lebih stabil)      |\n",
        "| **Boundary**    | Tajam, bisa overfit          | Lebih halus & smooth       |\n",
        "| **Stabilitas**  | Rentan terhadap noise        | Lebih tahan terhadap noise |\n",
        "| **Overfitting** | Mungkin                      | Lebih kecil kemungkinannya |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔚 Kesimpulan:\n",
        "\n",
        "* Decision Tree sangat **mudah ditafsirkan**, cocok untuk baseline model.\n",
        "* Namun, mereka **rentan overfitting dan tidak stabil**.\n",
        "* Untuk hasil produksi, **Random Forest** jauh lebih powerful karena mitigasi kelemahan di atas.\n",
        "* Visualisasi boundary dan regulasi sangat membantu dalam **eksplorasi awal data dan model**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LArRxlLJjPmk"
      }
    }
  ]
}