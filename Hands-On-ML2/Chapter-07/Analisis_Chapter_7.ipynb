{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY0FlhOQ4uv/hrocarsjhx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamdansyaif/DeepLearning/blob/main/Hands-On-ML2/Chapter-07/Analisis_Chapter_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# 📘 **Analisis Teori & Kode: Chapter 7 – Ensemble Learning**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 🗳️ **Voting Classifier**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* Voting adalah teknik sederhana untuk menggabungkan beberapa prediksi model.\n",
        "* **Hard voting** → berdasarkan mayoritas kelas.\n",
        "* **Soft voting** → berdasarkan probabilitas prediksi rata-rata.\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "VotingClassifier(estimators=[...], voting=\"hard\"/\"soft\")\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Cocok untuk model yang saling **komplementer**.\n",
        "* **VotingClassifier** efektif jika tiap model punya kinerja stabil dan tidak terlalu bias.\n",
        "* Dalam Exercise 8, **menghapus SVM** (karena performa buruk) bisa **meningkatkan akurasi ensemble**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🌲 **Bagging & Pasting**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* **Bagging (Bootstrap Aggregating):** Model dilatih pada subset data yang di-*sampling* dengan replacement.\n",
        "* **Pasting:** Sama dengan Bagging, tapi tanpa replacement.\n",
        "* Tujuan: menurunkan variance.\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "BaggingClassifier(estimator=DecisionTreeClassifier(...), n_estimators=500, bootstrap=True)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Membuat model lebih stabil dibanding satu Decision Tree.\n",
        "* Hasil decision boundary **lebih smooth** dan generalisasi lebih baik.\n",
        "* **OOB Score** (Out-of-Bag) dapat digunakan sebagai validasi otomatis → sangat efisien untuk evaluasi tanpa split tambahan.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🌳 **Random Forest**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* Random Forest = Bagging + Random Subset of Features saat split.\n",
        "* Ini memperkuat keanekaragaman model dan mengurangi korelasi antar pohon.\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Random Forest sangat powerful dan sering menjadi baseline default.\n",
        "* Prediksinya sangat mirip dengan **BaggingClassifier** jika parameternya disesuaikan.\n",
        "* Kita juga gunakan `.feature_importances_` untuk menganalisis **kontribusi fitur**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 🧠 **Feature Importance**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* Random Forest dapat memberikan skor pentingnya fitur berdasarkan kontribusi terhadap pengurangan impurity.\n",
        "* Ini sangat berguna untuk interpretasi dan seleksi fitur.\n",
        "\n",
        "### 🔹 *Visualisasi:*\n",
        "\n",
        "* Digunakan pada dataset **Iris** dan **MNIST** (untuk melihat pentingnya piksel).\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Fitur-fitur seperti **panjang dan lebar petal** di dataset Iris paling berkontribusi.\n",
        "* Untuk MNIST, piksel-piksel di tengah digit paling penting dalam prediksi angka.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. ⚡ **AdaBoost**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* **Boosting** adalah teknik sequential, tiap model belajar dari kesalahan model sebelumnya.\n",
        "* **AdaBoost:** Mengubah *weight* pada data; kesalahan besar jadi lebih penting di iterasi berikutnya.\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=200)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Cocok untuk model sederhana (mis. stump)\n",
        "* Model akhir jadi **lebih fokus pada kasus sulit**\n",
        "* Decision boundary tampak **lebih tajam** daripada Bagging\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 🪜 **Gradient Boosting (Manual & Sklearn)**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* Tiap model baru belajar dari **residu** (error dari model sebelumnya).\n",
        "* GBM = Optimisasi fungsi loss secara bertahap.\n",
        "* Learning rate + jumlah pohon → kendali atas over/underfitting.\n",
        "\n",
        "### 🔹 *Manual Implementation:*\n",
        "\n",
        "```python\n",
        "residual = y - tree1.predict(X)\n",
        "tree2.fit(X, residual)\n",
        "```\n",
        "\n",
        "### 🔹 *Sklearn:*\n",
        "\n",
        "```python\n",
        "GradientBoostingRegressor(n_estimators=200, learning_rate=0.1)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Gradient boosting **lebih fleksibel dan akurat** daripada AdaBoost.\n",
        "* Risiko overfitting tinggi jika `n_estimators` terlalu besar.\n",
        "* **Early stopping** efektif untuk membatasi jumlah pohon optimal.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. 🚀 **XGBoost**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* **Extreme Gradient Boosting**\n",
        "* Implementasi C++ yang cepat, scalable, dan mendukung regularisasi (L1/L2)\n",
        "* Populer di kompetisi Kaggle\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "xgboost.XGBRegressor(...).fit(...)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Performanya sangat baik, bahkan di dataset kecil seperti parabola regression.\n",
        "* Fitur `early_stopping_rounds` sangat membantu, dengan penyesuaian versi XGBoost (v1.7.6 aman).\n",
        "* Learning curve dan MSE rendah menunjukkan kemampuan adaptasi terhadap noise dan pola non-linear.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. 🧱 **Stacking (Manual Blender)**\n",
        "\n",
        "### 🔹 *Teori:*\n",
        "\n",
        "* Gabungkan prediksi beberapa model ke **meta-model (blender)**\n",
        "* Blender belajar **kombinasi optimal** dari output model-model base\n",
        "\n",
        "### 🔹 *Implementasi:*\n",
        "\n",
        "```python\n",
        "X_stack = np.c_[model1.predict(X), model2.predict(X), ...]\n",
        "blender.fit(X_stack, y)\n",
        "```\n",
        "\n",
        "### 🔹 *Analisis:*\n",
        "\n",
        "* Sering outperform voting karena belajar bagaimana menggabungkan prediksi.\n",
        "* Risiko: overfitting jika base model terlalu mirip.\n",
        "* Dalam Exercise 9, menggunakan Random Forest sebagai blender menunjukkan hasil OOB dan test akurasi sangat baik.\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ **Kesimpulan Umum**\n",
        "\n",
        "| Teknik            | Kelebihan                              | Kekurangan                               |\n",
        "| ----------------- | -------------------------------------- | ---------------------------------------- |\n",
        "| Voting            | Simple, cepat                          | Tidak adaptif, semua model dianggap sama |\n",
        "| Bagging           | Stabil, bisa pakai OOB                 | Butuh banyak model                       |\n",
        "| Random Forest     | Powerful, punya feature importance     | Tidak selalu optimal                     |\n",
        "| AdaBoost          | Fokus pada error                       | Rentan noise                             |\n",
        "| Gradient Boosting | Lebih presisi, bisa di-tune            | Overfit jika tak dikontrol               |\n",
        "| XGBoost           | Cepat, scalable, regularisasi built-in | Lebih kompleks konfigurasi               |\n",
        "| Stacking          | Potensi terbaik, sangat fleksibel      | Rentan overfit jika tak diatur baik      |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7az80SlJ0WD4"
      }
    }
  ]
}