{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrNrxPVWwlF++48sq4o7jM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamdansyaif/DeepLearning/blob/main/Week_8_UTS/Analisis_Regresi_UTS_Telkom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No 1\n",
        "Jika menggunakan model MLP dengan 3 hidden layer (256-128-64) menghasilkan underfitting pada dataset ini, modifikasi apa yang akan dilakukan pada arsitektur? Jelaskan alasan setiap perubahan dengan mempertimbangkan bias-variance tradeoff!\n",
        "---\n",
        "Jawab:\n",
        "Jika model **MLP dengan arsitektur 3 hidden layer (256-128-64)** mengalami **underfitting**, artinya model tidak mampu menangkap pola dari data dengan baik — baik pada data pelatihan maupun validasi. Dalam konteks **bias-variance tradeoff**, underfitting disebabkan oleh **bias yang tinggi**, yaitu model terlalu sederhana atau belum cukup fleksibel untuk mempelajari hubungan kompleks dalam data.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Strategi Modifikasi Arsitektur MLP untuk Mengatasi Underfitting:**\n",
        "\n",
        "Berikut adalah **modifikasi arsitektur** beserta **alasan logisnya** terkait bias-variance tradeoff:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 1. **Menambah Jumlah Neuron di Setiap Hidden Layer**\n",
        "\n",
        "* **Contoh:** dari (256-128-64) → menjadi (512-256-128)\n",
        "* **Alasan:** Dengan memperbesar kapasitas model, model dapat mempelajari pola yang lebih kompleks → **mengurangi bias**.\n",
        "* **Bias-Variance Tradeoff:** Menambah neuron akan menurunkan bias, tapi mungkin sedikit menaikkan varians (yang bisa dikendalikan dengan regularisasi).\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 2. **Menambah Jumlah Hidden Layer (Depth)**\n",
        "\n",
        "* **Contoh:** (512-256-128) → (512-256-128-64)\n",
        "* **Alasan:** Deep networks lebih mampu menangkap **representasi hierarkis** dari data (fitur-fitur kompleks), apalagi jika data memiliki struktur yang dalam.\n",
        "* **Bias-Variance Tradeoff:** Menurunkan bias karena meningkatkan kompleksitas model. Jika terlalu dalam, bisa meningkatkan varians → perlu regularisasi/dropout.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 3. **Mengganti atau Menambahkan Aktivasi Non-Linear yang Lebih Kompleks**\n",
        "\n",
        "* **Contoh:** ReLU → LeakyReLU atau ELU\n",
        "* **Alasan:** Beberapa fungsi aktivasi seperti LeakyReLU bisa menghindari **“dead neuron”** problem pada ReLU dan memperkuat representasi non-linear.\n",
        "* **Bias-Variance Tradeoff:** Memungkinkan representasi non-linear yang lebih baik → menurunkan bias.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 4. **Menurunkan Tingkat Regularisasi jika Terlalu Tinggi (L2, Dropout)**\n",
        "\n",
        "* **Contoh:** Dropout 0.5 → menjadi 0.3 atau bahkan dinonaktifkan sementara.\n",
        "* **Alasan:** Regularisasi yang terlalu kuat akan menghambat pembelajaran dan menyebabkan underfitting.\n",
        "* **Bias-Variance Tradeoff:** Mengurangi regularisasi akan menurunkan bias, namun perlu berhati-hati agar tidak meningkatkan varians terlalu banyak.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 5. **Meningkatkan Jumlah Epoch atau Mengurangi Early Stopping**\n",
        "\n",
        "* **Alasan:** Bisa jadi model belum selesai belajar karena training terlalu cepat dihentikan.\n",
        "* **Bias-Variance Tradeoff:** Membiarkan model belajar lebih lama bisa mengurangi bias, asalkan belum terjadi overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 6. **Feature Engineering atau Normalisasi yang Lebih Baik**\n",
        "\n",
        "* **Alasan:** Fitur yang belum relevan, atau belum ternormalisasi dengan baik dapat menyebabkan model tidak mampu belajar.\n",
        "* **Bias-Variance Tradeoff:** Bukan bagian dari arsitektur, tapi meningkatkan representasi fitur dapat membantu model mengurangi bias secara tidak langsung.\n",
        "\n",
        "---\n",
        "\n",
        "### ✍️ Ringkasan Rekomendasi Modifikasi:\n",
        "\n",
        "| Modifikasi         | Tujuan                           | Efek pada Bias-Variance   |\n",
        "| ------------------ | -------------------------------- | ------------------------- |\n",
        "| Tambah neuron      | Lebih banyak kapasitas           | ↓ Bias, ↑ Varians sedikit |\n",
        "| Tambah layer       | Representasi lebih dalam         | ↓ Bias                    |\n",
        "| Ganti aktivasi     | Lebih stabil dan kuat            | ↓ Bias                    |\n",
        "| Kurangi dropout/L2 | Mengurangi hambatan pembelajaran | ↓ Bias, ↑ Varians         |\n",
        "| Tambah epoch       | Pembelajaran lebih lengkap       | ↓ Bias                    |\n",
        "| Perbaiki fitur     | Input lebih informatif           | ↓ Bias                    |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WM8I4mpstOpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No 2\n",
        "Selain MSE, loss function apa yang mungkin cocok untuk dataset ini? Bandingkan kelebihan dan kekurangannya, serta situasi spesifik di mana alternatif tersebut lebih unggul daripada MSE!\n",
        "---\n",
        "Jawab:\n",
        "---\n",
        "\n",
        "## ✅ Alternatif Loss Function Selain MSE\n",
        "\n",
        "### 1. **MAE (Mean Absolute Error)**\n",
        "\n",
        "* **Rumus:**\n",
        "\n",
        "  $$\n",
        "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "  $$\n",
        "\n",
        "#### 🔹 **Kelebihan:**\n",
        "\n",
        "* Lebih **robust terhadap outlier** dibanding MSE.\n",
        "* Menghasilkan nilai loss yang **langsung mencerminkan rata-rata deviasi absolut**.\n",
        "\n",
        "#### 🔹 **Kekurangan:**\n",
        "\n",
        "* Gradiennya **konstan**, sehingga lebih lambat saat mendekati minimum.\n",
        "* Tidak memperbesar penalti untuk error besar → bisa kurang sensitif jika outlier penting.\n",
        "\n",
        "#### 📌 **Cocok digunakan jika:**\n",
        "\n",
        "* Data memiliki **outlier ekstrem** atau distribusi target yang skewed.\n",
        "* Tujuan akhir adalah **akurasi median** daripada kuadrat error.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Huber Loss**\n",
        "\n",
        "* Kombinasi antara MSE dan MAE.\n",
        "\n",
        "  $$\n",
        "  L_\\delta(y, \\hat{y}) =\n",
        "  \\begin{cases}\n",
        "  \\frac{1}{2}(y - \\hat{y})^2 & \\text{jika } |y - \\hat{y}| \\leq \\delta \\\\\n",
        "  \\delta(|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{jika } |y - \\hat{y}| > \\delta\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "#### 🔹 **Kelebihan:**\n",
        "\n",
        "* Menangani **outlier** lebih baik dari MSE.\n",
        "* Lebih **halus secara numerik** daripada MAE.\n",
        "* **Stabil saat training** dan tetap peka terhadap kesalahan kecil.\n",
        "\n",
        "#### 🔹 **Kekurangan:**\n",
        "\n",
        "* Butuh **hyperparameter δ** yang harus ditentukan.\n",
        "* Performa bisa tergantung nilai threshold δ.\n",
        "\n",
        "#### 📌 **Cocok digunakan jika:**\n",
        "\n",
        "* Dataset memiliki **beberapa outlier** tapi kita masih ingin penalti kuat untuk error kecil.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Log-Cosh Loss**\n",
        "\n",
        "* Fungsi loss halus yang mirip MSE saat error kecil, mirip MAE saat error besar.\n",
        "\n",
        "  $$\n",
        "  \\text{logcosh}(x) = \\log(\\cosh(x)) = \\log\\left(\\frac{e^x + e^{-x}}{2}\\right)\n",
        "  $$\n",
        "\n",
        "#### 🔹 **Kelebihan:**\n",
        "\n",
        "* **Smooth loss** function → lebih stabil saat backpropagation.\n",
        "* Penalti untuk error besar lebih rendah dari MSE (mirip MAE).\n",
        "\n",
        "#### 🔹 **Kekurangan:**\n",
        "\n",
        "* Tidak seintuitif MSE/MAE.\n",
        "* Kadang sedikit lebih lambat konvergensi karena fungsi log.\n",
        "\n",
        "#### 📌 **Cocok digunakan jika:**\n",
        "\n",
        "* Ingin menghindari efek keras dari MSE dan MAE, serta menjaga kelancaran optimisasi.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Perbandingan Ringkas:\n",
        "\n",
        "| Loss Function | Sensitivitas Outlier  | Penalti Error Besar    | Konvergensi            | Cocok Saat                         |\n",
        "| ------------- | --------------------- | ---------------------- | ---------------------- | ---------------------------------- |\n",
        "| **MSE**       | Tinggi (tidak robust) | Sangat besar           | Cepat (gradient besar) | Data bersih, error besar penting   |\n",
        "| **MAE**       | Rendah (robust)       | Linear                 | Lambat                 | Banyak outlier, distribusi skewed  |\n",
        "| **Huber**     | Sedang (terkontrol)   | Non-linear, adjustable | Stabil                 | Sedikit outlier, balance           |\n",
        "| **Log-Cosh**  | Rendah (soft)         | Smooth                 | Stabil                 | Butuh stabilitas dan fleksibilitas |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Kesimpulan untuk Dataset Ini:\n",
        "\n",
        "Jika kamu melihat pada hasil output model kamu sebelumnya dan melihat adanya:\n",
        "\n",
        "* **Outlier pada target**\n",
        "* **Distribusi target skewed**\n",
        "* **Model underfitting atau loss MSE tidak stabil**\n",
        "\n",
        "➡️ Maka **Huber Loss** adalah kandidat yang **paling direkomendasikan**, karena memberikan kompromi terbaik antara penalti kuat (MSE) dan robust (MAE).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tY_8Q2p6tTi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No 3\n",
        "Jika salah satu fitur memiliki range nilai 0-1, sedangkan fitur lain 100-1000, bagaimana ini memengaruhi pelatihan MLP? Jelaskan mekanisme matematis (e.g., gradien, weight update) yang terdampak!\n",
        "---\n",
        "Jawab:\n",
        "---\n",
        "\n",
        "## ⚠️ Masalah: Fitur Tidak Diskalakan (Misalnya: 0–1 vs. 100–1000)\n",
        "\n",
        "### ✅ Mekanisme yang Terpengaruh:\n",
        "\n",
        "### 1. **Forward Pass: Aktivasi Tidak Seimbang**\n",
        "\n",
        "* Jika satu fitur (misalnya `x1 ∈ [0,1]`) dan yang lain (`x2 ∈ [100,1000]`), maka:\n",
        "\n",
        "  $$\n",
        "  z = w_1 x_1 + w_2 x_2 + b\n",
        "  $$\n",
        "\n",
        "  akan **dominan oleh x2**, karena skalanya jauh lebih besar.\n",
        "\n",
        "> ➤ Ini menyebabkan **fitur skala kecil hampir diabaikan** oleh jaringan.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Backward Pass: Gradien Tidak Stabil**\n",
        "\n",
        "* Backpropagation menghitung turunan dari loss terhadap bobot:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial \\text{Loss}}{\\partial w_i} = \\frac{\\partial \\text{Loss}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i} = \\delta \\cdot x_i\n",
        "  $$\n",
        "\n",
        "> ➤ Jadi jika `x_i` besar (misal 1000), maka **gradien w\\_i akan besar**, menyebabkan **update besar** pada weight tersebut.\n",
        "\n",
        "Sebaliknya, fitur yang kecil menghasilkan **gradien kecil**, dan weight-nya **di-update lebih lambat** → tidak belajar optimal.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Masalah Vanishing/Exploding Gradients**\n",
        "\n",
        "* Dalam jaringan dalam (deep network), input yang tidak distandarkan bisa menyebabkan nilai aktivasi `z` sangat besar → menyebabkan **aktivasi sigmoid/tanh menjadi saturasi (mendekati 1 atau -1)**:\n",
        "\n",
        "  * Ini membuat turunan (`∂σ/∂z`) → mendekati **nol**\n",
        "  * Sehingga **gradien hilang (vanishing gradient)** dan **learning stagnan**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Batch Normalization Tidak Efektif**\n",
        "\n",
        "* Jika data belum dinormalisasi, maka distribusi input batch sangat bervariasi → **batch normalization** akan sering melakukan re-scaling besar-besaran, yang mengacaukan stabilitas training.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Dampak Praktis\n",
        "\n",
        "| Masalah                    | Akibat pada MLP Training                            |\n",
        "| -------------------------- | --------------------------------------------------- |\n",
        "| Fitur dengan skala besar   | Mendominasi prediksi, fitur kecil diabaikan         |\n",
        "| Gradien tidak proporsional | Learning tidak merata, bobot belajar tidak seimbang |\n",
        "| Optimizer tidak efisien    | Convergence lambat, bisa stuck pada suboptimal      |\n",
        "| Risiko vanishing gradient  | Model sulit belajar saat masuk hidden layers        |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solusi: Normalisasi atau Standardisasi Fitur\n",
        "\n",
        "1. **Min-Max Scaling:**\n",
        "   Semua fitur ke rentang \\[0,1]\n",
        "\n",
        "   $$\n",
        "   x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
        "   $$\n",
        "2. **Standard Scaling (Z-score):**\n",
        "   Buat mean = 0 dan std = 1\n",
        "\n",
        "   $$\n",
        "   x' = \\frac{x - \\mu}{\\sigma}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Kesimpulan\n",
        "\n",
        "> **Tanpa normalisasi fitur, training MLP menjadi tidak stabil karena gradien dan update bobot sangat dipengaruhi oleh skala fitur.** Hal ini mengarah ke konvergensi lambat, fitur tidak belajar, dan performa model rendah."
      ],
      "metadata": {
        "id": "ZQvcUTLMtU0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No 4\n",
        "Tanpa mengetahui nama fitur, bagaimana Anda mengukur kontribusi relatif setiap fitur terhadap prediksi model? Jelaskan metode teknikal (e.g., permutation importance, weight analysis) dan keterbatasannya!\n",
        "---\n",
        "Jawab:\n",
        "---\n",
        "\n",
        "## ✅ Metode 1: **Permutation Feature Importance (PFI)**\n",
        "\n",
        "### 🔧 Cara kerja:\n",
        "\n",
        "1. Evaluasi model dengan data asli → dapatkan skor (misal: MSE).\n",
        "2. Untuk setiap fitur:\n",
        "\n",
        "   * Acak nilainya (permutasi antar sampel).\n",
        "   * Ukur ulang performa model.\n",
        "   * Hitung selisih performa sebelum dan sesudah → semakin besar penurunan performa, semakin penting fitur tersebut.\n",
        "\n",
        "### 📌 Kelebihan:\n",
        "\n",
        "* **Model-agnostik** → bisa diterapkan di MLP, XGBoost, dll.\n",
        "* Bisa diterapkan meskipun **fitur tidak dinamai** (cukup pakai indeks kolom).\n",
        "\n",
        "### ⚠️ Keterbatasan:\n",
        "\n",
        "* **Komputasi mahal**, karena perlu inference berulang kali.\n",
        "* Tidak bisa menangkap **interaksi antar fitur** secara baik.\n",
        "* Bisa bias pada fitur yang memiliki korelasi tinggi → hasil overestimasi.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Metode 2: **Weight Magnitude Analysis (khusus MLP)**\n",
        "\n",
        "### 🔧 Cara kerja:\n",
        "\n",
        "* Ambil **weight dari input layer ke hidden layer pertama** (`W ∈ R^{n_input × n_hidden}`).\n",
        "* Hitung **norm** (misal: L2) dari bobot setiap input fitur ke seluruh neuron layer berikutnya:\n",
        "\n",
        "  $$\n",
        "  \\text{importance}_i = \\| W_{i,:} \\|_2\n",
        "  $$\n",
        "* Semakin besar norm-nya, semakin kuat pengaruh fitur tersebut.\n",
        "\n",
        "### 📌 Kelebihan:\n",
        "\n",
        "* **Cepat dan sederhana**.\n",
        "* Memberi insight langsung dari parameter model.\n",
        "\n",
        "### ⚠️ Keterbatasan:\n",
        "\n",
        "* Tidak mempertimbangkan **aktivasi atau nonlinearitas** (ReLU, dsb).\n",
        "* Tidak memperhitungkan **output atau loss** secara langsung.\n",
        "* **Bisa misleading**: bobot besar ≠ penting, karena bisa dikompensasi oleh bobot layer berikutnya.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Metode 3: **SHAP (SHapley Additive exPlanations)**\n",
        "\n",
        "### 🔧 Cara kerja:\n",
        "\n",
        "* Menghitung **kontribusi marjinal** dari tiap fitur terhadap prediksi, berdasarkan teori game (Shapley Value).\n",
        "* Bisa digunakan untuk deep learning melalui `DeepExplainer` (TensorFlow/Keras) atau `KernelExplainer`.\n",
        "\n",
        "### 📌 Kelebihan:\n",
        "\n",
        "* Menyediakan **penjelasan lokal dan global**.\n",
        "* Menghitung **interaksi fitur** secara adil.\n",
        "\n",
        "### ⚠️ Keterbatasan:\n",
        "\n",
        "* **Mahal secara komputasi** untuk banyak fitur.\n",
        "* Implementasi untuk PyTorch perlu wrapper eksternal (`captum`, `shap`), dan tidak trivial tanpa nama fitur.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Kesimpulan\n",
        "\n",
        "| Metode                 | Bisa Tanpa Nama Fitur | Akurat | Cepat | Kelebihan Utama                           |\n",
        "| ---------------------- | --------------------- | ------ | ----- | ----------------------------------------- |\n",
        "| Permutation Importance | ✅                     | ✅      | ❌     | Interpretasi model-agnostik yang solid    |\n",
        "| Weight Norm Analysis   | ✅                     | ❌      | ✅     | Cepat, bisa diterapkan langsung           |\n",
        "| SHAP                   | ✅ (pakai index)       | ✅✅     | ❌❌    | Interpretasi mendalam berbasis teori game |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "N5GY1D_9tVQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PofO4mGTtWOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No 5\n",
        "Bagaimana Anda mendesain eksperimen untuk memilih learning rate dan batch size secara optimal? Sertakan analisis tradeoff antara komputasi dan stabilitas pelatihan!\n",
        "---\n",
        "Jawab:\n",
        "---\n",
        "\n",
        "### ✅ 1. **Grid Search / Random Search Eksploratif**\n",
        "\n",
        "#### 🔧 Langkah-langkah:\n",
        "\n",
        "1. **Tentukan range**:\n",
        "\n",
        "   * Learning rate (LR): \\[1e-4, 1e-3, 1e-2, 1e-1]\n",
        "   * Batch size: \\[16, 32, 64, 128, 256]\n",
        "2. Untuk setiap kombinasi:\n",
        "\n",
        "   * Latih model selama beberapa epoch tetap (misalnya 20).\n",
        "   * Catat metrik validasi: loss, akurasi, atau MSE.\n",
        "   * Simpan model terbaik berdasarkan metrik validasi.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 2. **Learning Rate Finder (LR range test)**\n",
        "\n",
        "#### 🔧 Langkah-langkah:\n",
        "\n",
        "1. Uji training dari learning rate sangat kecil (misal 1e-7) → besar (misal 1).\n",
        "2. Set batch size tetap (misal 64).\n",
        "3. Catat loss terhadap learning rate (log-scale).\n",
        "4. Pilih LR optimal di **titik sebelum loss mulai naik drastis** (biasanya di lereng curam awal grafik).\n",
        "\n",
        "#### 📌 Kelebihan:\n",
        "\n",
        "* Cepat, hanya 1 epoch.\n",
        "* Memberi gambaran jelas LR yang efektif.\n",
        "\n",
        "#### ⚠️ Kelemahan:\n",
        "\n",
        "* Tidak mempertimbangkan batch size.\n",
        "* Implementasi perlu visualisasi tambahan.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 3. **Grid/Random Search dengan Early Stopping**\n",
        "\n",
        "* Kombinasikan **Grid/Random Search** dengan **early stopping** untuk mempercepat waktu eksperimen dan menghindari overfitting.\n",
        "* Cocok jika model kompleks dan eksperimen lama.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Analisis Tradeoff\n",
        "\n",
        "| Parameter         | Kecil (misal 1e-5, batch 16)                                              | Besar (misal 1e-1, batch 256)                                                          |\n",
        "| ----------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
        "| **Learning Rate** | ✅ Lebih stabil, pelatihan lebih presisi<br>❌ Lambat konvergensi           | ✅ Konvergen cepat<br>❌ Risiko overshooting, divergen                                   |\n",
        "| **Batch Size**    | ✅ Noisy update → lebih baik generalisasi<br>❌ Lebih lambat & tidak stabil | ✅ Update stabil & cepat (lebih cocok untuk GPU)<br>❌ Bisa overfit, kurang generalisasi |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Rekomendasi Praktis\n",
        "\n",
        "| Tujuan                          | Strategi                                                                     |\n",
        "| ------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| Cepat konvergen awal            | Gunakan **learning rate finder**, lalu fix batch size (misal 64)             |\n",
        "| Akurasi maksimum jangka panjang | Gunakan **Grid Search kecil** + early stopping                               |\n",
        "| Sumber daya terbatas            | **Random Search** 10–20 kombinasi → lebih hemat daripada Grid Search lengkap |\n",
        "| Stabilitas penting              | Mulai dari batch size kecil, LR rendah → naik bertahap jika loss stabil      |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3NeOKxjhtXTS"
      }
    }
  ]
}